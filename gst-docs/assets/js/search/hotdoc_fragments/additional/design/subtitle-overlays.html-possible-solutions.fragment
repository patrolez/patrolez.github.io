fragment_downloaded_cb({"url":"additional/design/subtitle-overlays.html#possible-solutions","fragment":"Possible Solutions\nThe goal is to keep knowledge of the subtitle format within the\nformat-specific GStreamer plugins, and knowledge of any specific video\nacceleration API to the GStreamer plugins implementing that API. We do\nnot want to make the pango/dvbsuboverlay/dvdspu/kate plugins link to\nlibva/libvdpau/etc. and we do not want to make the vaapi/vdpau plugins\nlink to all of libpango/libkate/libass etc.\nMultiple possible solutions come to mind:\nbackend-specific overlay elements\ne.g. vaapitextoverlay, vdpautextoverlay, vaapidvdspu, vdpaudvdspu,\nvaapidvbsuboverlay, vdpaudvbsuboverlay, etc.\nThis assumes the overlay can be done directly on the\nbackend-specific object passed around.\nThe main drawback with this solution is that it leads to a lot of\ncode duplication and may also lead to uncertainty about distributing\ncertain duplicated pieces of code. The code duplication is pretty\nmuch unavoidable, since making textoverlay, dvbsuboverlay, dvdspu,\nkate, assrender, etc. available in form of base classes to derive\nfrom is not really an option. Similarly, one would not really want\nthe vaapi/vdpau plugin to depend on a bunch of other libraries such\nas libpango, libkate, libtiger, libass, etc.\nOne could add some new kind of overlay plugin feature though in\ncombination with a generic base class of some sort, but in order to\naccommodate all the different cases and formats one would end up\nwith quite convoluted/tricky API.\n(Of course there could also be a GstFancyVideoBuffer that provides\nan abstraction for such video accelerated objects and that could\nprovide an API to add overlays to it in a generic way, but in the\nend this is just a less generic variant of (c), and it is not clear\nthat there are real benefits to a specialised solution vs. a more\ngeneric one).\nconvert backend-specific object to raw pixels and then overlay\nEven where possible technically, this is most likely very\ninefficient.\nattach the overlay data to the backend-specific video frame buffers\nin a generic way and do the actual overlaying/blitting later in\nbackend-specific code such as the video sink (or an accelerated\nencoder/transcoder)\nIn this case, the actual overlay rendering (i.e. the actual text\nrendering or decoding DVD/DVB data into pixels) is done in the\nsubtitle-format-specific GStreamer plugin. All knowledge about the\nsubtitle format is contained in the overlay plugin then, and all\nknowledge about the video backend in the video backend specific\nplugin.\nThe main question then is how to get the overlay pixels (and we will\nonly deal with pixels here) from the overlay element to the video\nsink.\nThis could be done in multiple ways: One could send custom events\ndownstream with the overlay data, or one could attach the overlay\ndata directly to the video buffers in some way.\nSending inline events has the advantage that is fairly\ntransparent to any elements between the overlay element and the\nvideo sink: if an effects plugin creates a new video buffer for the\noutput, nothing special needs to be done to maintain the subtitle\noverlay information, since the overlay data is not attached to the\nbuffer. However, it slightly complicates things at the sink, since\nit would also need to look for the new event in question instead of\njust processing everything in its buffer render function.\nIf one attaches the overlay data to the buffer directly, any element\nbetween overlay and video sink that creates a new video buffer would\nneed to be aware of the overlay data attached to it and copy it over\nto the newly-created buffer.\nOne would have to do implement a special kind of new query (e.g.\nFEATURE query) that is not passed on automatically by\ngst_pad_query_default() in order to make sure that all elements\ndownstream will handle the attached overlay data. (This is only a\nproblem if we want to also attach overlay data to raw video pixel\nbuffers; for new non-raw types we can just make it mandatory and\nassume support and be done with it; for existing non-raw types\nnothing changes anyway if subtitles don't work) (we need to maintain\nbackwards compatibility for existing raw video pipelines like e.g.:\n..decoder ! suboverlay ! encoder..)\nEven though slightly more work, attaching the overlay information to\nbuffers seems more intuitive than sending it interleaved as events.\nAnd buffers stored or passed around (e.g. via the \"last-buffer\"\nproperty in the sink when doing screenshots via playbin) always\ncontain all the information needed.\ncreate a video/x-raw-*-delta format and use a backend-specific\nvideomixer\nThis possibility was hinted at already in the digression in section\nOverall (c) appears to be the most promising solution. It is the least\nintrusive and should be fairly straight-forward to implement with\nreasonable effort, requiring only small changes to existing elements and\nrequiring no new elements.\nDoing the final overlaying in the sink as opposed to a videomixer or\noverlay in the middle of the pipeline has other advantages:\nif video frames need to be dropped, e.g. for QoS reasons, we could\nalso skip the actual subtitle overlaying and possibly the\ndecoding/rendering as well, if the implementation and API allows for\nthat to be delayed.\nthe sink often knows the actual size of the window/surface/screen\nthe output video is rendered to. This may make it possible to\nrender the overlay image in a higher resolution than the input\nvideo, solving a long standing issue with pixelated subtitles on top\nof low-resolution videos that are then scaled up in the sink. This\nwould require for the rendering to be delayed of course instead of\njust attaching an AYUV/ARGB/RGBA blog of pixels to the video buffer\nin the overlay, but that could all be supported.\nif the video backend / sink has support for high-quality text\nrendering (clutter?) we could just pass the text or pango markup to\nthe sink and let it do the rest (this is unlikely to be supported in\nthe general case - text and glyph rendering is hard; also, we don't\nreally want to make up our own text markup system, and pango markup\nis probably too limited for complex karaoke stuff).\n\n\nif video frames need to be dropped, e.g. for QoS reasons, we could\nalso skip the actual subtitle overlaying and possibly the\ndecoding/rendering as well, if the implementation and API allows for\nthat to be delayed.\n\n\nthe sink often knows the actual size of the window/surface/screen\nthe output video is rendered to. This may make it possible to\nrender the overlay image in a higher resolution than the input\nvideo, solving a long standing issue with pixelated subtitles on top\nof low-resolution videos that are then scaled up in the sink. This\nwould require for the rendering to be delayed of course instead of\njust attaching an AYUV/ARGB/RGBA blog of pixels to the video buffer\nin the overlay, but that could all be supported.\n\n\nif the video backend / sink has support for high-quality text\nrendering (clutter?) we could just pass the text or pango markup to\nthe sink and let it do the rest (this is unlikely to be supported in\nthe general case - text and glyph rendering is hard; also, we don't\nreally want to make up our own text markup system, and pango markup\nis probably too limited for complex karaoke stuff).\n\n\n"});