fragment_downloaded_cb({"url":"additional/design/subtitle-overlays.html#api-needed","fragment":"API needed\nRepresentation of subtitle overlays to be rendered\nWe need to pass the overlay pixels from the overlay element to the\nsink somehow. Whatever the exact mechanism, let's assume we pass a\nrefcounted GstVideoOverlayComposition struct or object.\nA composition is made up of one or more overlays/rectangles.\nIn the simplest case an overlay rectangle is just a blob of\nRGBA/ABGR [FIXME?] or AYUV pixels with positioning info and other\nmetadata, and there is only one rectangle to render.\nWe're keeping the naming generic (\"OverlayFoo\" rather than\n\"SubtitleFoo\") here, since this might also be handy for other use\ncases such as e.g. logo overlays or so. It is not designed for\nfull-fledged video stream mixing\nthough.\nFallback overlay rendering/blitting on top of raw video\nEventually we want to use this overlay mechanism not only for\nhardware-accelerated video, but also for plain old raw video, either\nat the sink or in the overlay element directly.\nApart from the advantages listed earlier in section 3, this allows\nus to consolidate a lot of overlaying/blitting code that is\ncurrently repeated in every single overlay element in one location.\nThis makes it considerably easier to support a whole range of raw\nvideo formats out of the box, add SIMD-optimised rendering using\nORC, or handle corner cases correctly.\n(Note: side-effect of overlaying raw video at the video sink is that\nif e.g. a screnshotter gets the last buffer via the last-buffer\nproperty of basesink, it would get an image without the subtitles on\ntop. This could probably be fixed by re-implementing the property in\nGstVideoSink though. Playbin2 could handle this internally as well).\nFlatten all rectangles in a composition\nWe cannot assume that the video backend API can handle any number of\nrectangle overlays, it's possible that it only supports one single\noverlay, in which case we need to squash all rectangles into one.\nHowever, we'll just declare this a corner case for now, and\nimplement it only if someone actually needs it. It's easy to add\nlater API-wise. Might be a bit tricky if we have rectangles with\ndifferent PARs/formats (e.g. subs and a logo), though we could\nprobably always just use the code from (b) with a fully transparent\nvideo buffer to create a flattened overlay buffer.\nquery support for the new video composition mechanism\nThis is handled via GstMeta and an ALLOCATION query - we can simply\nquery whether downstream supports the GstVideoOverlayComposition meta.\nThere appears to be no issue with downstream possibly not being\nlinked yet at the time when an overlay would want to do such a\nquery, but we would just have to default to something and update\nourselves later on a reconfigure event then.\nOther considerations:\nrenderers (overlays or sinks) may be able to handle only ARGB or\nonly AYUV (for most graphics/hw-API it's likely ARGB of some sort,\nwhile our blending utility functions will likely want the same\ncolour space as the underlying raw video format, which is usually\nYUV of some sort). We need to convert where required, and should\ncache the conversion.\nrenderers may or may not be able to scale the overlay. We need to do\nthe scaling internally if not (simple case: just horizontal scaling\nto adjust for PAR differences; complex case: both horizontal and\nvertical scaling, e.g. if subs come from a different source than the\nvideo or the video has been rescaled or cropped between overlay\nelement and sink).\nrenderers may be able to generate (possibly scaled) pixels on demand\nfrom the original data (e.g. a string or RLE-encoded data). We will\nignore this for now, since this functionality can still be added\nlater via API additions. The most interesting case would be to pass\na pango markup string, since e.g. clutter can handle that natively.\nrenderers may be able to write data directly on top of the video\npixels (instead of creating an intermediary buffer with the overlay\nwhich is then blended on top of the actual video frame), e.g.\ndvdspu, dvbsuboverlay\nHowever, in the interest of simplicity, we should probably ignore the\nfact that some elements can blend their overlays directly on top of the\nvideo (decoding/uncompressing them on the fly), even more so as it's not\nobvious that it's actually faster to decode the same overlay 70-90 times\n(say) (ie. ca. 3 seconds of video frames) and then blend it 70-90 times\ninstead of decoding it once into a temporary buffer and then blending it\ndirectly from there, possibly SIMD-accelerated. Also, this is only\nrelevant if the video is raw video and not some hardware-acceleration\nbackend object.\nAnd ultimately it is the overlay element that decides whether to do the\noverlay right there and then or have the sink do it (if supported). It\ncould decide to keep doing the overlay itself for raw video and only use\nour new API for non-raw video.\nrenderers may want to make sure they only upload the overlay pixels\nonce per rectangle if that rectangle recurs in subsequent frames (as\npart of the same composition or a different composition), as is\nlikely. This caching of e.g. surfaces needs to be done renderer-side\nand can be accomplished based on the sequence numbers. The\ncomposition contains the lowest sequence number still in use\nupstream (an overlay element may want to cache created\ncompositions+rectangles as well after all to re-use them for\nmultiple frames), based on that the renderer can expire cached\nobjects. The caching needs to be done renderer-side because\nattaching renderer-specific objects to the rectangles won't work\nwell given the refcounted nature of rectangles and compositions,\nmaking it unpredictable when a rectangle or composition will be\nfreed or from which thread context it will be freed. The\nrenderer-specific objects are likely bound to other types of\nrenderer-specific contexts, and need to be managed in connection\nwith those.\ncomposition/rectangles should internally provide a certain degree of\nthread-safety. Multiple elements (sinks, overlay element) might\naccess or use the same objects from multiple threads at the same\ntime, and it is expected that elements will keep a ref to\ncompositions and rectangles they push downstream for a while, e.g.\nuntil the current subtitle composition expires.\n\n\nrenderers (overlays or sinks) may be able to handle only ARGB or\nonly AYUV (for most graphics/hw-API it's likely ARGB of some sort,\nwhile our blending utility functions will likely want the same\ncolour space as the underlying raw video format, which is usually\nYUV of some sort). We need to convert where required, and should\ncache the conversion.\n\n\nrenderers may or may not be able to scale the overlay. We need to do\nthe scaling internally if not (simple case: just horizontal scaling\nto adjust for PAR differences; complex case: both horizontal and\nvertical scaling, e.g. if subs come from a different source than the\nvideo or the video has been rescaled or cropped between overlay\nelement and sink).\n\n\nrenderers may be able to generate (possibly scaled) pixels on demand\nfrom the original data (e.g. a string or RLE-encoded data). We will\nignore this for now, since this functionality can still be added\nlater via API additions. The most interesting case would be to pass\na pango markup string, since e.g. clutter can handle that natively.\n\n\nrenderers may be able to write data directly on top of the video\npixels (instead of creating an intermediary buffer with the overlay\nwhich is then blended on top of the actual video frame), e.g.\ndvdspu, dvbsuboverlay\n\n\n\n\nrenderers may want to make sure they only upload the overlay pixels\nonce per rectangle if that rectangle recurs in subsequent frames (as\npart of the same composition or a different composition), as is\nlikely. This caching of e.g. surfaces needs to be done renderer-side\nand can be accomplished based on the sequence numbers. The\ncomposition contains the lowest sequence number still in use\nupstream (an overlay element may want to cache created\ncompositions+rectangles as well after all to re-use them for\nmultiple frames), based on that the renderer can expire cached\nobjects. The caching needs to be done renderer-side because\nattaching renderer-specific objects to the rectangles won't work\nwell given the refcounted nature of rectangles and compositions,\nmaking it unpredictable when a rectangle or composition will be\nfreed or from which thread context it will be freed. The\nrenderer-specific objects are likely bound to other types of\nrenderer-specific contexts, and need to be managed in connection\nwith those.\n\n\ncomposition/rectangles should internally provide a certain degree of\nthread-safety. Multiple elements (sinks, overlay element) might\naccess or use the same objects from multiple threads at the same\ntime, and it is expected that elements will keep a ref to\ncompositions and rectangles they push downstream for a while, e.g.\nuntil the current subtitle composition expires.\n\n\n"});